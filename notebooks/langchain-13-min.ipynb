{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabbitmetrics/langchain-13-min/blob/main/notebooks/langchain-13-min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nLarge language models are neural networks trained on large datasets of text to predict the next word in a sequence of words.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "llm(\"explain large language models in one sentence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JtHgQ5XpJmgi"
      },
      "outputs": [],
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
        "\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yrfYfKfdJyyF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here's an example of a Python script that trains a neural network on simulated data using the TensorFlow library:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "\n",
            "# Generate simulated data\n",
            "np.random.seed(0)\n",
            "X = np.random.rand(100, 2)\n",
            "y = np.random.randint(0, 2, size=(100,))\n",
            "\n",
            "# Define the neural network architecture\n",
            "model = tf.keras.models.Sequential([\n",
            "    tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),\n",
            "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
            "])\n",
            "\n",
            "# Compile the model\n",
            "model.compile(optimizer='adam',\n",
            "              loss='binary_crossentropy',\n",
            "              metrics=['accuracy'])\n",
            "\n",
            "# Train the model\n",
            "model.fit(X, y, epochs=10, batch_size=32)\n",
            "\n",
            "# Evaluate the model\n",
            "loss, accuracy = model.evaluate(X, y)\n",
            "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
            "```\n",
            "\n",
            "In this script, we first generate simulated data using `numpy`. We then define a simple neural network architecture using `tf.keras.models.Sequential`. The network consists of two dense layers, with 10 units and ReLU activation in the first layer, and 1 unit and sigmoid activation in the second layer.\n",
            "\n",
            "Next, we compile the model using `model.compile`, specifying the optimizer, loss function, and metrics to track during training. In this case, we use the Adam optimizer and binary cross-entropy loss.\n",
            "\n",
            "We then train the model using `model.fit`, passing in the simulated data `X` and `y`, as well as the number of epochs and batch size.\n",
            "\n",
            "Finally, we evaluate the trained model on the same simulated data using `model.evaluate`, and print the loss and accuracy.\n"
          ]
        }
      ],
      "source": [
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "response=chat(messages)\n",
        "\n",
        "print(response.content,end='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "outputs": [],
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vcz7Q9Y-KFvI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAn autoencoder is a type of neural network architecture that is used to compress data by learning to represent the data in a lower-dimensional space. It consists of an encoder, which maps the input data to a compressed representation, and a decoder, which reconstructs the input data from the compressed representation.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm(prompt.format(concept=\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "An autoencoder is a type of artificial neural network used to learn efficient data representations (i.e. encodings) by training the network to ignore signal noise. It works by compressing the input data into a smaller representation, which is then reconstructed back to its original form. Autoencoders are used for data compression, denoising, and visualization.\n"
          ]
        }
      ],
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B6MF4-nMKul3"
      },
      "outputs": [],
      "source": [
        "# Define a second prompt \n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SkJKFyk1K-MO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "An autoencoder is an unsupervised deep learning technique used to learn a representation of data by encoding it into a lower dimensional space. It is composed of an encoder and decoder that compress and decompress the data, and is trained by reconstructing the input from its compressed representation.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "An autoencoder is like a machine that is designed to help you learn about data. It’s like a teacher who can teach you some new information. \n",
            "\n",
            "First, the autoencoder takes the data and compresses it. This is like taking a big pile of information and making it smaller. It’s like squishing all of the data into a tiny little box. \n",
            "\n",
            "Next, the autoencoder takes that compressed data and then it decompresses it. This is like taking the tiny little box and making it bigger again. \n",
            "\n",
            "After the autoencoder has compressed and decompressed the data, it is trained. This means that it learns how to do the compression and decompression better and better. It’s like teaching the teacher how to be a better teacher. \n",
            "\n",
            "Finally, once the autoencoder is trained, it can take the data and reconstruct it from its compressed representation. This is like taking the data that was squished into the tiny little box and making it big again. \n",
            "\n",
            "In summary, an autoencoder is a machine that takes data, compresses it into a small box, decompresses it to make it bigger, and\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "An autoencoder is like a machine that is designed to help you learn about data. It’s like a teacher who can teach you some new information. \n",
            "\n",
            "First, the autoencoder takes the data and compresses it. This is like taking a big pile of information and making it smaller. It’s like squishing all of the data into a tiny little box. \n",
            "\n",
            "Next, the autoencoder takes that compressed data and then it decompresses it. This is like taking the tiny little box and making it bigger again. \n",
            "\n",
            "After the autoencoder has compressed and decompressed the data, it is trained. This means that it learns how to do the compression and decompression better and better. It’s like teaching the teacher how to be a better teacher. \n",
            "\n",
            "Finally, once the autoencoder is trained, it can take the data and reconstruct it from its compressed representation. This is like taking the data that was squished into the tiny little box and making it big again. \n",
            "\n",
            "In summary, an autoencoder is a machine that takes data, compresses it into a small box, decompresses it to make it bigger, and\n"
          ]
        }
      ],
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "outputs": [],
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [],
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "outputs": [],
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "outputs": [],
      "source": [
        "# Import and initialize Pinecone client\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
        "    environment=os.getenv('PINECONE_ENV')  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "outputs": [],
      "source": [
        "# Upload vectors to Pinecone\n",
        "\n",
        "index_name = \"langchain-quickstart\"\n",
        "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "outputs": [],
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = search.similarity_search(query)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ogz8luZNRnJ"
      },
      "outputs": [],
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHMDj0sNi09"
      },
      "outputs": [],
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
